### YamlMime:ManagedReference
items:
- uid: System.Speech.Recognition.RecognizedAudio
  id: RecognizedAudio
  children:
  - System.Speech.Recognition.RecognizedAudio.AudioPosition
  - System.Speech.Recognition.RecognizedAudio.Duration
  - System.Speech.Recognition.RecognizedAudio.Format
  - System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  - System.Speech.Recognition.RecognizedAudio.StartTime
  - System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  - System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  langs:
  - csharp
  name: RecognizedAudio
  nameWithType: RecognizedAudio
  fullName: System.Speech.Recognition.RecognizedAudio
  type: Class
  summary: "表示音频输入，它是与<xref href=&quot;System.Speech.Recognition.RecognitionResult&quot;> </xref>。"
  remarks: "语音识别器生成的音频输入有关识别操作的一部分的信息。 若要访问识别的音频，使用<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>属性或<xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A>方法的<xref:System.Speech.Recognition.RecognitionResult>。</xref:System.Speech.Recognition.RecognitionResult> </xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> </xref:System.Speech.Recognition.RecognitionResult.Audio%2A>       以下事件和方法可以生成识别结果<xref:System.Speech.Recognition.SpeechRecognizer>和<xref:System.Speech.Recognition.SpeechRecognitionEngine>类:-事件:-<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>的方法:-<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>1> [!IMPORTANT]&1;> A 识别由模拟的语音识别的结果不包含可识别的音频。</xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine> </xref:System.Speech.Recognition.SpeechRecognizer> 此类识别结果，其<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>属性返回`null`及其<xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A>方法引发异常。</xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> </xref:System.Speech.Recognition.RecognitionResult.Audio%2A> 有关模拟的语音识别的详细信息，请参阅`EmulateRecognize`和`EmulateRecognizeAsync`方法<xref:System.Speech.Recognition.SpeechRecognizer>和<xref:System.Speech.Recognition.SpeechRecognitionEngine>类。</xref:System.Speech.Recognition.SpeechRecognitionEngine> </xref:System.Speech.Recognition.SpeechRecognizer>"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>, or <xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public class RecognizedAudio
  inheritance:
  - System.Object
  implements: []
  inheritedMembers: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition
  id: AudioPosition
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "为识别音频 start 获取输入音频流中的位置。"
  remarks: "此属性引用所识别的短语中的输入的设备生成音频流开头的位置。 与此相反，`RecognizerAudioPosition`属性<xref:System.Speech.Recognition.SpeechRecognitionEngine>和<xref:System.Speech.Recognition.SpeechRecognizer>类引用其音频的输入内识别器的位置。</xref:System.Speech.Recognition.SpeechRecognizer> </xref:System.Speech.Recognition.SpeechRecognitionEngine> 这些位置可以是不同的。 有关详细信息，请参阅[使用语音识别事件](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482)。       <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>属性获取系统时间的识别操作的开头。</xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public TimeSpan AudioPosition { get; }
    return:
      type: System.TimeSpan
      description: "识别音频中输入音频流开头的位置。"
  overload: System.Speech.Recognition.RecognizedAudio.AudioPosition*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.Duration
  id: Duration
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: Duration
  nameWithType: RecognizedAudio.Duration
  fullName: System.Speech.Recognition.RecognizedAudio.Duration
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "获取识别音频输入音频流的持续时间。"
  remarks: ''
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public TimeSpan Duration { get; }
    return:
      type: System.TimeSpan
      description: "识别音频输入音频流中的时间。"
  overload: System.Speech.Recognition.RecognizedAudio.Duration*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.Format
  id: Format
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: Format
  nameWithType: RecognizedAudio.Format
  fullName: System.Speech.Recognition.RecognizedAudio.Format
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "获取由识别引擎处理的音频格式。"
  remarks: ''
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public System.Speech.AudioFormat.SpeechAudioFormatInfo Format { get; }
    return:
      type: System.Speech.AudioFormat.SpeechAudioFormatInfo
      description: "由语音识别器处理的音频格式。"
  overload: System.Speech.Recognition.RecognizedAudio.Format*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  id: GetRange(System.TimeSpan,System.TimeSpan)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: GetRange(TimeSpan,TimeSpan)
  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "选择并返回当前识别中的一部分音频为二进制数据。"
  remarks: ''
  example:
  - "The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \n  \n```  \nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \n{  \n  GrammarBuilder builder = new GrammarBuilder();  \n  builder.Append(\"My name is\");  \n  builder.AppendWildcard();  \n  \n  Grammar nameGrammar = new Grammar(builder);  \n  nameGrammar.Name = \"Name Grammar\";  \n  nameGrammar.SpeechRecognized +=  \n    new EventHandler<SpeechRecognizedEventArgs>(  \n      NameSpeechRecognized);  \n  \n  recognizer.LoadGrammar(nameGrammar);  \n}  \n  \n// Handle the SpeechRecognized event of the name grammar.  \nprivate static void NameSpeechRecognized(  \n  object sender, SpeechRecognizedEventArgs e)  \n{  \n  Console.WriteLine(\"Grammar ({0}) recognized speech: {1}\",  \n    e.Result.Grammar.Name, e.Result.Text);  \n  \n  try  \n  {  \n  \n    // The name phrase starts after the first three words.  \n    if (e.Result.Words.Count < 4)  \n    {  \n  \n      // Add code to check for an alternate that contains the wildcard.  \n      return;  \n    }  \n  \n    RecognizedAudio audio = e.Result.Audio;  \n    TimeSpan start = e.Result.Words[3].AudioPosition;  \n    TimeSpan duration = audio.Duration - start;  \n  \n    // Add code to verify and persist the audio.  \n    string path = @\"C:\\temp\\nameAudio.wav\";  \n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \n    {  \n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \n      nameAudio.WriteToWaveStream(outputStream);  \n      outputStream.Close();  \n    }  \n  \n    Thread testThread =  \n      new Thread(new ParameterizedThreadStart(TestAudio));  \n    testThread.Start(path);  \n  }  \n  catch (Exception ex)  \n  {  \n    Console.WriteLine(\"Exception thrown while processing audio:\");  \n    Console.WriteLine(ex.ToString());  \n  }  \n}  \n  \n// Use the speech synthesizer to play back the .wav file  \n// that was created in the SpeechRecognized event handler.  \n  \nprivate static void TestAudio(object item)  \n{  \n  string path = item as string;  \n  if (path != null && File.Exists(path))  \n  {  \n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \n    PromptBuilder builder = new PromptBuilder();  \n    builder.AppendText(\"Hello\");  \n    builder.AppendAudio(path);  \n    synthesizer.Speak(builder);  \n  }  \n}  \n```"
  syntax:
    content: public System.Speech.Recognition.RecognizedAudio GetRange (TimeSpan audioPosition, TimeSpan duration);
    parameters:
    - id: audioPosition
      type: System.TimeSpan
      description: "要返回的音频数据起始点。"
    - id: duration
      type: System.TimeSpan
      description: "要返回的段的长度。"
    return:
      type: System.Speech.Recognition.RecognizedAudio
      description: "按照定义返回的子部分的可识别音频<code> audioPosition </code>和<code> duration </code>。"
  overload: System.Speech.Recognition.RecognizedAudio.GetRange*
  exceptions:
  - type: System.ArgumentOutOfRangeException
    commentId: T:System.ArgumentOutOfRangeException
    description: "<code>audioPosition</code>和<code>duration</code>定义超出当前段的范围的音频的段。"
  - type: System.InvalidOperationException
    commentId: T:System.InvalidOperationException
    description: "当前识别音频不包含任何数据。"
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.StartTime
  id: StartTime
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
  fullName: System.Speech.Recognition.RecognizedAudio.StartTime
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "在开始时识别操作获取系统时间。"
  remarks: "在开始时识别操作，这会非常有用的延迟和性能的计算，StartTime 属性获取系统时间。       <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A>属性获取中的输入的设备生成音频流的位置。</xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A>"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public DateTime StartTime { get; }
    return:
      type: System.DateTime
      description: "识别操作开始时系统时间。"
  overload: System.Speech.Recognition.RecognizedAudio.StartTime*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  id: WriteToAudioStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: WriteToAudioStream(Stream)
  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "将整个音频作为原始数据写入流。"
  remarks: "音频数据写入到`outputStream`以二进制形式。 不不包括任何标头信息。       WriteToAudioStream 方法使用批格式，但不包括批标头。 若要包括批标头，使用<xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A>方法。</xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A>"
  syntax:
    content: public void WriteToAudioStream (System.IO.Stream outputStream);
    parameters:
    - id: outputStream
      type: System.IO.Stream
      description: "将接收音频数据的流。"
  overload: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  id: WriteToWaveStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: WriteToWaveStream(Stream)
  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "音频写入波形格式的流。"
  remarks: "音频数据写入到`outputStream`批格式，其中包括资源交换文件格式 (RIFF) 标头。       <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>方法使用相同的二进制格式，但不包括批标头。</xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>"
  example:
  - "The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \n  \n```  \nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \n{  \n  GrammarBuilder builder = new GrammarBuilder();  \n  builder.Append(\"My name is\");  \n  builder.AppendWildcard();  \n  \n  Grammar nameGrammar = new Grammar(builder);  \n  nameGrammar.Name = \"Name Grammar\";  \n  nameGrammar.SpeechRecognized +=  \n    new EventHandler<SpeechRecognizedEventArgs>(  \n      NameSpeechRecognized);  \n  \n  recognizer.LoadGrammar(nameGrammar);  \n}  \n  \n// Handle the SpeechRecognized event of the name grammar.  \nprivate static void NameSpeechRecognized(  \n  object sender, SpeechRecognizedEventArgs e)  \n{  \n  Console.WriteLine(\"Grammar ({0}) recognized speech: {1}\",  \n    e.Result.Grammar.Name, e.Result.Text);  \n  \n  try  \n  {  \n    // The name phrase starts after the first three words.  \n    if (e.Result.Words.Count < 4)  \n    {  \n  \n      // Add code to check for an alternate that contains the   \nwildcard.  \n      return;  \n    }  \n  \n    RecognizedAudio audio = e.Result.Audio;  \n    TimeSpan start = e.Result.Words[3].AudioPosition;  \n    TimeSpan duration = audio.Duration - start;  \n  \n    // Add code to verify and persist the audio.  \n    string path = @\"C:\\temp\\nameAudio.wav\";  \n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \n    {  \n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \n      nameAudio.WriteToWaveStream(outputStream);  \n      outputStream.Close();  \n    }  \n  \n    Thread testThread =  \n      new Thread(new ParameterizedThreadStart(TestAudio));  \n    testThread.Start(path);  \n  }  \n  catch (Exception ex)  \n  {  \n    Console.WriteLine(\"Exception thrown while processing audio:\");  \n    Console.WriteLine(ex.ToString());  \n  }  \n}  \n  \n// Use the speech synthesizer to play back the .wav file  \n// that was created in the SpeechRecognized event handler.  \n  \nprivate static void TestAudio(object item)  \n{  \n  string path = item as string;  \n  if (path != null && File.Exists(path))  \n  {  \n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \n    PromptBuilder builder = new PromptBuilder();  \n    builder.AppendText(\"Hello\");  \n    builder.AppendAudio(path);  \n    synthesizer.Speak(builder);  \n  }  \n}  \n```"
  syntax:
    content: public void WriteToWaveStream (System.IO.Stream outputStream);
    parameters:
    - id: outputStream
      type: System.IO.Stream
      description: "将接收音频数据的流。"
  overload: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*
  exceptions: []
  platform:
  - net462
references:
- uid: System.Object
  isExternal: false
  name: System.Object
- uid: System.ArgumentOutOfRangeException
  isExternal: true
  name: System.ArgumentOutOfRangeException
- uid: System.InvalidOperationException
  isExternal: true
  name: System.InvalidOperationException
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition
- uid: System.TimeSpan
  parent: System
  isExternal: true
  name: TimeSpan
  nameWithType: TimeSpan
  fullName: System.TimeSpan
- uid: System.Speech.Recognition.RecognizedAudio.Duration
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Duration
  nameWithType: RecognizedAudio.Duration
  fullName: System.Speech.Recognition.RecognizedAudio.Duration
- uid: System.Speech.Recognition.RecognizedAudio.Format
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Format
  nameWithType: RecognizedAudio.Format
  fullName: System.Speech.Recognition.RecognizedAudio.Format
- uid: System.Speech.AudioFormat.SpeechAudioFormatInfo
  parent: System.Speech.AudioFormat
  isExternal: false
  name: SpeechAudioFormatInfo
  nameWithType: SpeechAudioFormatInfo
  fullName: System.Speech.AudioFormat.SpeechAudioFormatInfo
- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: GetRange(TimeSpan,TimeSpan)
  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)
- uid: System.Speech.Recognition.RecognizedAudio
  parent: System.Speech.Recognition
  isExternal: false
  name: RecognizedAudio
  nameWithType: RecognizedAudio
  fullName: System.Speech.Recognition.RecognizedAudio
- uid: System.Speech.Recognition.RecognizedAudio.StartTime
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
  fullName: System.Speech.Recognition.RecognizedAudio.StartTime
- uid: System.DateTime
  parent: System
  isExternal: true
  name: DateTime
  nameWithType: DateTime
  fullName: System.DateTime
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToAudioStream(Stream)
  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)
- uid: System.IO.Stream
  parent: System.IO
  isExternal: true
  name: Stream
  nameWithType: Stream
  fullName: System.IO.Stream
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToWaveStream(Stream)
  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
- uid: System.Speech.Recognition.RecognizedAudio.Duration*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Duration
  nameWithType: RecognizedAudio.Duration
- uid: System.Speech.Recognition.RecognizedAudio.Format*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Format
  nameWithType: RecognizedAudio.Format
- uid: System.Speech.Recognition.RecognizedAudio.GetRange*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: GetRange
  nameWithType: RecognizedAudio.GetRange
- uid: System.Speech.Recognition.RecognizedAudio.StartTime*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToAudioStream
  nameWithType: RecognizedAudio.WriteToAudioStream
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToWaveStream
  nameWithType: RecognizedAudio.WriteToWaveStream
